---
title: 算法时间空间复杂度分析
tags:
  - 算法
  - 时间复杂度
  - 空间复杂度
abbrlink: 4079050735
date: 2020-05-05 21:07:03
---
数据结构和算法本质上来说是解决快和省的问题。快通常指的是速度快，省一般指的是空间省。
也就是说，如何让代码运行得更快，让代码更省空间，是算法主要解决的问题
因此今天就讲一下时间、空间复杂度分析

## 为什么需要复杂度分析

在做项目的时候，我们只需要把代码跑一遍，通过监控数据，或者看看示波器（看看IO的翻转），就能得到算法执行的时间和占用内存的大小了啊。难道有什么东西比实战更有效更准确吗？

实际上，这种方法是可行的。但是这种统计方法具有非常大的局限性，只能反映出当前的数据是正确的，但是没办法反映永远都是正确的。

### 这种测试方法非常依赖于我们的测试环境
实际上，测试环境对这种方法影响极大。
比如我们写了一段插入排序的代码。我们用MCU（STM32/51）和CPU（Intel Core i9）来运行。
不用说，CPU肯定比MCU执行的速度要快很多。
而且放在不同的编译环境，编译器对它的优化也有一定的影响

### 这种测试方法对数据的大小非常敏感
同样的使用排序算法，比如从大到小那样排，如果排列的顺序不一样或者数据的长度不一样的话，排序的执行时间就会有很大的差别。比如我们在最好的情况下，我们这个排序不需要做任何操作，执行时间非常短。另外如果数据量比较小的话，不管这个排序数列是有序还是无序的，可能看起来都不一样。如果对于小规模的数列，插入排序可能比快排要快。

### 结论
实测确实能反映出当前的算法情况，但是它非常依赖于我们的测试环境和测试的数据。
思考一下，我们能不能不需要实测（实测需要大量的时间），就可以估算出算法的执行效率。
这就是今天要学习的时间、空间复杂度分析法

## 大O复杂度表示法
算法的执行效率，简单来说就是代码执行的时间。
那么我们怎么才能估算出一段代码的执行时间呢？
下面有段代码，求1 ~ n的累加：

``` bash
int myCalculation(int n)
{
    int sum = 0;
    int i;
    for(i = 1;i <= n;i++){
        sum += i;
    }
    return sum
}
```

对于CPU而言，都是读内存数据 - 运算 - 写内存数据。
不同的编译环境下，每行代码执行的时间都是不一样的。假设每行代码执行的时间都是 1个周期，
那么 第2-3行执行的时间是 2个周期，第4-5行分别运行了n个周期，2行就是2n个周期，所以代码的总执行时间为2n + 2个周期。
由上可见，代码总执行时间和代码执行的次数有关

再看看下面这段代码:

``` bash
int myCalculation()
{
    int sum = 0;
    int i = 0;
    int j = 0;
    for(i = 1;i<=n;i++){
        j = 1;
        for(;j<=n;++j){
            sum = sum + i * j;
        }
    }
}
```
继续分析：
1）2 ~ 4 行一共需要运行 3个周期的时间
2）5,6行一共需要2n个周期的时间
3）7,8行一共需要n*n个周期的时间
综上所述，整段代码的总运行时间为: 3 + 2n + n*n周期
由上可见，代码的总执行时间和代码执行次数成正比

当我们把这个规律总结成公式，那就是我们的大O分析法出场的时候了
T(n) = O(f(n))
其中T(n)代表的是代码执行的时间，n代表的是我们数据大小（如代码执行的次数）
f(n)代表每行代码执行次数的总和。
O表示代码执行的时间和f(n)成正比

总结上面两个例子，第一个例子中T(n) = O(2n + 2),第二个例子中T(n) = O(2n*n + 2n + 3)这就是大O时间复杂度表示法。
但是大O时间复杂度实际上并不能表示代码真正的执行时间，而是表示代码执行时间随着数据规模增长的变化趋势，简称时间复杂度
当这个n很大的时候。此时公式中其他的数据并不会影响到它增长的趋势的话，那么都可以忽略。我们只需要记录一个最大量级就可以了。
在这种情况下，第一个例子中就是T(n) = O(n),第二个例子中就是T(n) = O(n * n)

## 时间复杂度分析
三种方法告诉你，怎么去分析一段代码的时间复杂度

### 只关注循环执行次数最多的一段代码
大O这种复杂度表示方法只是表示一种变化趋势，我们一般忽略掉公式中的一些常量，只需要记录最大阶的量级就可以了。所以我们在分析一个算法的时间复杂度的时候，也只需要关注循环执行次数最大的那一段代码就可以了。这种核心代码执行次数的N的量级，就是整段要分析代码的时间复杂度

拿例子1 来进行说明：
因为前面两句临时变量的定义都是常量级的执行时间，和N的大小无关，对复杂度没有影响。而循环执行次数最多的是4、5行代码。因为这两行代码被执行了N次，所以总的时间复杂度是O(n)

### 加法法则：总复杂度等于量级最大的那段代码的复杂度

``` bash
int cal(int n) {
   int sum_1 = 0;
   int p = 1;
   for (; p < 100; ++p) {
     sum_1 = sum_1 + p;
   }

   int sum_2 = 0;
   int q = 1;
   for (; q < n; ++q) {
     sum_2 = sum_2 + q;
   }
 
   int sum_3 = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1; 
     for (; j <= n; ++j) {
       sum_3 = sum_3 +  i * j;
     }
   }
 
   return sum_1 + sum_2 + sum_3;
 }
```
## 分析时间复杂度
sum_1的代码循环执行了100次，所以是一个常量的执行时间，跟n的规模无关
sum_2时间复杂度是O(n)
sum_3时间复杂度是O(n*n)
综合上面三个代码的时间复杂度，我们取最大的量级。
所以整段代码的时间复杂度是O(n*n),也就是说，总的时间复杂度就等于量级最大的那段代码的时间复杂度。
设 T1(n) = O(f(n)),T2(n) = O(g(n)),
那么T(n) = T1(n) + T2(n) = max (O(f(n)),O(g(n)))

### 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积
复杂度的乘法法则类似与加法法则那样
设 T1(n) = O(f(n)),T2(n) = O(g(n)),
那么T(n) = T1(n) * T2(n) = O(f(n)) * O(g(n))
同样的
``` bash
int cal(int n) {
   int ret = 0; 
   int i = 1;
   for (; i < n; ++i) {
     ret = ret + f(i);
   } 
 } 
 
 int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
    sum = sum + i;
  } 
  return sum;
 }

```
整个函数的时间复杂度为：T(n) = O(n*n)

## 多项式时间复杂度
### O(1)
O(1)不是代表我们只运行一次，它是常量级时间复杂度的一种表示方法
比如下面这段代码
``` bash
int i = 0;
int j = 0;
int sum = i + j + 100 + 20;

```
它的时间复杂度是O(1)
只要代码的执行时间不会随着n的增大而增大，这样代码的时间复杂度我们都记作O(1)

### O(logn)/O(nlogn)
对数阶时间复杂度非常常见，同时也是最难区别的一种时间复杂度
举个例子
``` bash
i = 1；
while(i <= n)
{
    i *= 2;
}
```
根据上面说的复杂度方法，while循环执行的次数是最多的，所以我们要得到时间复杂度，我们只需要计算出这几行代码一共执行了多少次就可以了。

逐步分析:
变量从i = 1开始自增，每一次循环就乘以2，等到大于等于n的时候，循环结束。
实际上，这就是我们高中的等比数列
i = 1
i = 2
i = 4
i = 8
...
i >=n
所以我们只需要知道最大的值是多少就行了
也就是计算一个等式:2的x次方 = n
所以代码的时间复杂度就是O(log2 n);

Next..
``` bash
i = 1；
while(i <= n)
{
    i *= 3;
}
```
代码的时间复杂度就是O(log3 n);
实际上，不管是以2为底还是以3为底，我们都可以把所有对数阶的时间复杂度都记为 O(logn)，why?

我们知道，对数之间是可以互相转换的，log3 n就等于 log3 2 * log2 n,所以O(log3 n) = O(C *log3 n)。而C = log3 2，这是一个常量。基于我们前面的一个理论，在采用大O计算复杂度的时候，可以忽略该系数，也就是O(C(f(n))) = O(f(n))
所以，O(log2 n) = O(log3 n);因此我们统一表示为 O(logn)
因此如果一段代码的时间复杂度是O(log n)的话，如果它循环执行了n次，那么时间复杂度说就是n(logn)，比如归并排序，快速排序

### O(m + n)/O(m*n)
我们再来讲一种和前面都不一样的时间复杂度，代码的复杂度由两个数据的规模来决定。

``` bash
int cal(int m, int n) {
  int sum_1 = 0;
  int i = 1;
  for (; i < m; ++i) {
    sum_1 = sum_1 + i;
  }

  int sum_2 = 0;
  int j = 1;
  for (; j < n; ++j) {
    sum_2 = sum_2 + j;
  }

  return sum_1 + sum_2;
}
```
从代码可以看出，m和n表示的是两个数据规模，我们没办法说m 和 n的差别是多少
所以我们不能抛弃m或者n，所以上面代码的时间复杂度是O(m + n)
这个时候加法法则的计算需要加点料了
T1(m) + T2(n) = O(f(m) + f(n))
乘法法则不变:
T1(m) * T2(n) = O(f(m) * f(n))

## 空间复杂度分析
时间复杂度是渐进时间复杂度，表示算法的执行时间和数据规模之间的增长关系
类比一下，空间复杂度就是渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系

``` bash
void print(int n) {
  int i = 0;
  int[] a = new int[n];
  for (i; i <n; ++i) {
    a[i] = i * i;
  }

  for (i = n-1; i >= 0; --i) {
    print out a[i]
  }
}
```
根据空间复杂度分析，因为第三行申请了一个大小为n的int 类型的数组。除此之外，代码没有做任何关于空间的操作，所以代码的空间复杂度是O(n)

## 总结
复杂度包括时间复杂度和空间复杂度，用于分析算法执行效率和数据规模之间的增长关系
可以认为，越高阶复杂度的算法，执行效率越低
从低阶-高阶：O(1) - O(log n) - O(n) - O(nlog n) - O(n * n)
复杂度分析分为以下几个方面：
1.单段代码计算时间复杂度 -- 看最高频的
2.多段代码计算时间复杂度 -- 看最大的一个复杂度（加法法则）
3.嵌套代码计算时间复杂度 -- 求乘积 （乘法法则）
4.多种情况计算时间复杂度 -- 复杂度相加/复杂度相乘


